Markdown# VideoRAG: Semantic Video Analysis & Keyframe Extraction**VideoRAG** is a Python-based pipeline designed to facilitate Retrieval-Augmented Generation (RAG) on video content. It automates the process of downloading YouTube videos, extracting frames, generating semantic embeddings using state-of-the-art vision models (SigLIP), and selecting the most representative keyframes via clustering.This tool is ideal for developers building "Chat with Video" applications, video summarization tools, or semantic video search engines.## Features* **YouTube Integration**: Seamlessly download videos using `yt_dlp`.* **State-of-the-Art Embeddings**: Uses **Google's SigLIP** (`siglip-so400m-patch14-384`) for high-quality, language-aligned image embeddings.* **Smart Keyframe Selection**: Implements **K-Means clustering** to select semantically diverse keyframes, ensuring the selected frames represent distinct scenes rather than repetitive shots.* **GPU Acceleration**: Automatically utilizes CUDA if available for faster processing.* **Visualization**: Built-in support for displaying frames using `matplotlib` and `seaborn-image`.## PrerequisitesEnsure you have Python 3.8+ installed. You will need the following libraries:```bashpip install numpy matplotlib seaborn-image scikit-learn torch transformers yt_dlp opencv-pythonNote: For GPU support, ensure you have the appropriate version of PyTorch installed for your CUDA version. InstallationClone this repository:Bashgit clone [https://github.com/yourusername/videorag.git](https://github.com/yourusername/videorag.git)cd videoragInstall dependencies (if not already done):Bashpip install -r requirements.txt(Create a requirements.txt with the libraries listed above if you haven't already) Usage1. Setup the PipelineThe core logic relies on the extract_keyframes function, which handles loading the model, processing the video, and clustering the frames.Pythonimport torchfrom transformers import AutoModel, AutoProcessorfrom sklearn.cluster import KMeansimport cv2import numpy as np# Load Model (Google SigLIP)device = torch.device("cuda" if torch.cuda.is_available() else "cpu")model_name = "google/siglip-so400m-patch14-384"model = AutoModel.from_pretrained(model_name).to(device)processor = AutoProcessor.from_pretrained(model_name, use_fast=True)2. Download and Process a VideoYou can download a video directly from YouTube and extract keyframes in one flow.Pythonimport yt_dlp# Download Videolink = "[https://www.youtube.com/watch?v=YOUR_VIDEO_ID](https://www.youtube.com/watch?v=YOUR_VIDEO_ID)"outdir = "./backend/download"ydl_opts = { "outtmpl": f"{outdir}/video.mp4", "format": "best[ext=mp4][acodec!=none]/best", "noplaylist": True}with yt_dlp.YoutubeDL(ydl_opts) as ydl: ydl.download([link])# Extract Keyframes# fps_sample: Sample 1 frame every X seconds (or use 1 for 1 FPS)# clusters: Number of keyframes to returnkeyframes, embeddings = extract_keyframes( video_path=f"{outdir}/video.mp4", fps_sample=1, clusters=20)print(f"Extracted {len(keyframes)} diverse keyframes.") How It WorksFrame Extraction: The script reads the video file and samples frames at a specified interval (e.g., 1 frame per second).Embedding Generation: Each sampled frame is passed through the SigLIP model to create a dense vector representation. SigLIP is chosen for its strong zero-shot performance and alignment with textual descriptions.Clustering: The embeddings are grouped using K-Means clustering.Selection: The algorithm identifies the frame closest to the centroid of each cluster. This ensures that the final list of keyframes covers the "story" of the video without redundancy. ContributingContributions are welcome! Please feel free to submit a Pull Request. LicenseMIT License
